# LLM-MultiSource-QA
This repository contains the code, dataset, and findings from our research on evaluating and improving Large Language Models (LLMs) for multi-source question-answering tasks. We explore various prompting strategies, custom evaluation metrics, and advanced techniques such as few-shot learning and chain of thought to enhance model performance.
